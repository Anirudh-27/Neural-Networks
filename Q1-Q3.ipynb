{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q1-Q3.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1eKijrMozhDKALrL4rdgQa8_tMcqXUiOt","authorship_tag":"ABX9TyOir/Lt5Lu0+XDS6OVtipjf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1Px93SHB1j1k"},"source":["# Q1 - Non Linear Perceptron"]},{"cell_type":"code","metadata":{"id":"v7F-I_Eb1fLv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606837791660,"user_tz":-330,"elapsed":1394,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"824140f4-63e8-4e9f-a1a6-4ecbab8818a6"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","file='/content/drive/MyDrive/2018AAPS1242H_NNFL (Assignment 2)/Data/data55.xlsx'\n","A=pd.read_excel(file,header=None)\n","d=np.asarray(A,dtype='float')\n","np.random.seed(10)\n","d=np.random.permutation(d)\n","o=np.zeros((len(d),1))\n","o[:,0]=d[:,4]\n","\n","d=np.delete(d,4,1)\n","x1mean=d[:,0].mean()\n","x2mean=d[:,1].mean()\n","x3mean=d[:,2].mean()\n","x4mean=d[:,3].mean()\n","x1std=np.std(d[:,0])\n","x2std=np.std(d[:,1])\n","x3std=np.std(d[:,2])\n","x4std=np.std(d[:,3])\n","#normalising training data\n","d[:,0]=(d[:,0]-d[:,0].mean())/np.std(d[:,0])\n","d[:,1]=(d[:,1]-d[:,1].mean())/np.std(d[:,1])\n","d[:,2]=(d[:,2]-d[:,2].mean())/np.std(d[:,2])\n","d[:,3]=(d[:,3]-d[:,3].mean())/np.std(d[:,3])\n","#hold out cross validation\n","x=d[:70]\n","v=d[70:80]\n","t=d[80:]\n","y=o[:70]\n","vo=o[70:80]\n","to=o[80:]\n","\n","k=100\n","a=0.5\n","\n","#initialising w and b with gaussian distribution\n","w=np.random.normal(0,1,size=(1,4))\n","b=np.random.normal(0,1,size=(1,1))\n","h=np.zeros((len(x),1))\n","lsq=np.zeros((k,1))\n","def sigmoid(sum):\n","  h=1/(1+math.exp(-sum))\n","  return h\n","def cost(h):\n","  j=0.5*np.sum(h-y)**2\n","  return j\n","for i in range(k):\n","  for j in range(len(x)):\n","    sum=np.sum(w@x[j].T)+b\n","    h[j]=sigmoid(sum)\n","    if h[j]>0.5:\n","      yp=1\n","    elif h[j]<0.5 or h[j]==0.5:\n","      yp=0\n","    if y[j]!=yp:\n","      w+=a*y[j]*x[j]\n","      b+=a*y[j]\n","  lsq[i]=cost(h)\n","c=0\n","h=np.zeros((len(v),1))\n","for i in range(len(v)):\n","  sum=np.sum(w@v[i].T)+b\n","  h[i]=sigmoid(sum)\n","  if h[i]>0.5:\n","      yp=1\n","  elif h[i]<0.5 or h[i]==0.5:\n","    yp=0\n","  if vo[i]==yp:\n","    c+=1\n","print(\"the validation accuracy is\",c/len(v))\n","\n","c=0\n","sen=0\n","spe=0\n","h=np.zeros((len(t),1))\n","for i in range(len(t)):\n","  sum=np.sum(w@t[i].T)+b\n","  h[i]=sigmoid(sum)\n","  if h[i]>0.5:\n","      yp=1\n","  elif h[i]<0.5 or h[i]==0.5:\n","    yp=0\n","  if to[i]==yp:\n","    c+=1\n","  if yp==1 and to[i]==1:\n","    sen+=1\n","  if yp==0 and to[i]==0:\n","    spe+=1\n","pos=0\n","for i in range(len(to)):\n","  if to[i]==1:\n","    pos+=1\n","neg=len(to)-pos\n","print(\"the test accuracy is\",c/len(t))\n","print(\"the sensitivity is\",sen/pos)\n","print(\"the specivity is\",spe/neg)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["the validation accuracy is 1.0\n","the test accuracy is 1.0\n","the sensitivity is 1.0\n","the specivity is 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CBEL3VTN2IIb"},"source":["# Q2 - Kernel Perceptron"]},{"cell_type":"code","metadata":{"id":"kUxpWEBE2Omo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606837780949,"user_tz":-330,"elapsed":1527,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"cd11a83e-316f-47e0-972b-358d72e3c0d0"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","file='/content/drive/MyDrive/2018AAPS1242H_NNFL (Assignment 2)/Data/data55.xlsx'\n","A=pd.read_excel(file,header=None)\n","d=np.asarray(A,dtype='float')\n","np.random.seed(20)\n","d=np.random.permutation(d)\n","o=np.zeros((len(d),1))\n","o[:,0]=d[:,4]\n","for i in range(len(o)):\n","  if o[i]==0:\n","    o[i]=-1\n","d=np.delete(d,4,1)\n","#normalising training data\n","d=(d-d.min(axis=0))/(d.max(axis=0)-d.min(axis=0))\n","\n","#hold out cross validation\n","x=d[:70]\n","v=d[70:80]\n","t=d[80:]\n","y=o[:70]\n","vo=o[70:80]\n","to=o[80:]\n","k=100\n","a=np.zeros((len(x),1))\n","def polyker(u,m):\n","  ker=np.zeros((len(m),len(u)))\n","  for i in range(len(m)):\n","    for j in range(len(u)):\n","      ker[i,j]=(1+np.dot(m[i],u[j]))**3\n","\n","  return ker\n","\n","ker=polyker(x,x)\n","\n","for i in range(k):\n","  for j in range(len(x)):\n","    if (np.sum(a*ker))!=0:\n","      yp=np.sign(np.sum(a*ker)*y[j]) #polynomial kernel\n","    else:\n","      yp=-1\n","    if yp!=y[j]:\n","      a[j]+=1\n","c=0\n","ker1=polyker(v,x)\n","\n","for i in range(len(v)):\n","  if np.sum(a*ker1)!=0:\n","    yp=np.sign(np.sum(a*ker1)*vo[i])\n","  else:\n","    yp=-1\n","  if yp==vo[i]:\n","    c+=1\n","print(\"the validation set accuracy is:\",c/len(vo))\n","c=0\n","sen=0\n","spe=0\n","ker2=polyker(t,x)\n","for i in range(len(t)):\n","  \n","  if np.sum(a*ker2)!=0:\n","    yp=np.sign(np.sum(a*ker2)*to[i])\n","  else:\n","    yp=-1\n","  if yp==to[i]:\n","    c+=1\n","  if yp==1 and to[i]==1:\n","    sen+=1\n","  if yp==-1 and to[i]==-1:\n","    spe+=1\n","pos=0\n","for i in range(len(t)):\n","  if to[i]==1:\n","    pos+=1\n","neg=len(to)-pos\n","print(\"the test accuracy is\",c/len(t))\n","print(\"the sensitivity is\",sen/pos)\n","print(\"the specivity is\",spe/neg)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["the validation set accuracy is: 1.0\n","the test accuracy is 1.0\n","the sensitivity is 1.0\n","the specivity is 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tSIghtx_2eaS"},"source":["# Q3 - Multilayer Perceptron"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ms_ZGamEXOUj","executionInfo":{"status":"ok","timestamp":1606837733815,"user_tz":-330,"elapsed":143558,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"d202ecac-dc03-4779-ecb8-7964dacfeb81"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.io import loadmat\n","\n","mat_contents = loadmat('/content/drive/MyDrive/2018AAPS1242H_NNFL (Assignment 2)/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)\n","\n","def init_data():\n","    X = np.array(data[:2148, :-1], dtype = float)\n","    y = np.array(data[:2148, -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","def affine_forward(x, w, b):\n","    z = x.dot(w) + b\n","    cache = (x, w, b)\n","    return z, cache\n","\n","def relu_forward(x):\n","    a = x\n","    a[a<=0] = 0\n","    cache = x\n","    return a, cache\n","\n","def affine_backward(dout, cache):\n","    x, w, b = cache\n","    db = np.sum(dout, axis = 0)\n","    dw = x.T.dot(dout)\n","    dx = dout.dot(w.T)\n","    return dx, dw, db\n","\n","def relu_backward(dout, cache):\n","    x = cache\n","    dx = None\n","    dx = np.ones(x.shape)\n","    dx[x<=0] = 0\n","    dx = dx * dout\n","    return dx\n","\n","class Twonet(object):\n","\n","    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, std=1e-4):\n","        self.W1 = std * np.random.randn(input_size, hidden_size1)\n","        self.b1 = np.zeros(hidden_size1)\n","        self.W2 = std * np.random.randn(hidden_size1, hidden_size2)\n","        self.b2 = np.zeros(hidden_size2)\n","        self.W3 = std * np.random.randn(hidden_size2, num_classes)\n","        self.b3 = np.zeros(num_classes)\n","\n","    def loss(self, X, y = None, reg = 0.0):\n","        N, D = X.shape\n","        scores = None\n","        z1, af_cache1 = affine_forward(X, self.W1, self.b1)\n","        h1, relu_cache1 = relu_forward(z1)\n","        z2, af_cache2 = affine_forward(h1, self.W2, self.b2)\n","        h2, relu_cache2 = relu_forward(z2)\n","        z3, af_cache3 = affine_forward(h2, self.W3, self.b3)\n","        scores = z3\n","\n","        if y is None:\n","            return scores\n","\n","        loss = None\n","        scores -= scores.max()\n","        scores_exp = np.exp(scores)\n","        correct_scores = scores[range(N), y]\n","        correct_scores_exp = np.exp(correct_scores)\n","        loss = np.sum(-np.log(correct_scores_exp / np.sum(scores_exp, axis = 1))) / N\n","        loss += 0.5 * reg * (np.sum(self.W1 * self.W1) + \\\n","            np.sum(self.W2 * self.W2) + np.sum(self.W3 * self.W3))\n","\n","        num = correct_scores_exp\n","        denom = np.sum(scores_exp, axis = 1)\n","        mask = (np.exp(z3)/denom.reshape(scores.shape[0],1))\n","        mask[range(N),y] = -(denom - num)/denom\n","        mask /= N\n","        dz3 = mask\n","\n","        dh2, dw3, db3 = affine_backward(dz3, af_cache3)\n","        dz2 = relu_backward(dh2, relu_cache2)\n","        dh1, dw2, db2 = affine_backward(dz2, af_cache2)\n","        dz1 = relu_backward(dh1, relu_cache1)\n","        dx, dw1, db1 = affine_backward(dz1, af_cache1)\n","        \n","        dw3 = dw3 + reg * self.W3\n","        dw2 = dw2 + reg * self.W2\n","        dw1 = dw1 + reg * self.W1\n","\n","        wgrad = (dw1, dw2, dw3)\n","        bgrad = (db1, db2, db3)\n","\n","        return loss, wgrad, bgrad\n","\n","    def train(self, X, y, X_val, y_val, alpha = 1e-3, alpha_decay = 0.95,\\\n","         reg = 5e-6, num_iters = 100, batch_size = 200):\n","        num_train = X.shape[0]\n","        iterations_per_epoch = max(num_train / batch_size, 1)\n","        loss_history = []\n","        train_acc_history = []\n","        val_acc_history = []\n","\n","        for it in range(num_iters):\n","\n","            ind = np.random.choice(num_train, batch_size)\n","            X_batch = X[ind,:]\n","            y_batch = y[ind]\n","            \n","            loss, wgrad, bgrad = self.loss(X_batch, y = y_batch, reg = reg)\n","            loss_history.append(loss)\n","\n","            dw1, dw2, dw3 = wgrad\n","            db1, db2, db3 = bgrad\n","\n","            self.W1 -= alpha * dw1\n","            self.W2 -= alpha * dw2\n","            self.W3 -= alpha * dw3\n","            self.b1 -= alpha * db1\n","            self.b2 -= alpha * db2\n","            self.b3 -= alpha * db3\n","\n","\n","            if it % 100 == 0:\n","                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\n","            if it % iterations_per_epoch == 0:\n","                train_acc = (self.predict(X_batch) == y_batch).mean()\n","                val_acc = (self.predict(X_val) == y_val).mean()\n","                train_acc_history.append(train_acc)\n","                val_acc_history.append(val_acc)\n","\n","                alpha *= alpha_decay\n","\n","\n","        return {'loss_history' : loss_history, 'train_acc_history' : \\\n","            train_acc_history, 'val_acc_history' : val_acc_history}\n","\n","\n","    def predict(self, X):\n","        y_pred = np.argmax(self.loss(X), axis = 1)\n","        return y_pred\n","\n","\n","\n","input_size = 72\n","hidden_size1 = 30\n","hidden_size2 = 30\n","num_classes = 2\n","num_inputs = 1790\n","std = 0.1\n","alpha = 0.3\n","split = 358\n","batch_size = 1024\n","reg = 1e-2\n","num_iters = 5000\n","\n","X_tot, y_tot = init_data()\n","\n","train_acc , val_acc = 0, 0\n","losses = np.empty((5, num_iters))\n","val_accs = []\n","train_accs = []\n","\n","for k in range(5):\n","    \n","    X = X_tot[0 : 1790]\n","    y = y_tot[0 : 1790]\n","    X_val = X_tot[1790 :]\n","    y_val = y_tot[1790 :]\n","    \n","    Net = Twonet(input_size, hidden_size1, hidden_size2, num_classes, std)\n","    print(\"Validation fold : \" , k + 1)\n","    stats = Net.train(X, y, X_val, y_val, num_iters = num_iters,\\\n","         alpha = alpha, batch_size = batch_size, reg = 0.0)\n","    losses[k] = np.asarray(stats['loss_history'])\n","    val_accs = np.asarray(stats['val_acc_history'])\n","    train_accs = np.asarray(stats['train_acc_history'])\n","    train_acc += train_accs\n","    val_acc += val_accs\n","\n","\n","    X_tot[0 : split] = X_val\n","    X_tot[split : ] = X\n","    y_tot[0 : split] = y_val\n","    y_tot[split : ] = y\n","\n","train_acc /= 5\n","val_acc /= 5\n","\n","print(train_acc[-1], val_acc[-1])\n","loss_hist = np.mean(losses, axis = 0)\n","\n","plt.subplot(2, 1, 1)\n","plt.plot(loss_hist)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.xlabel('Epoch')\n","plt.ylabel('Classification accuracy')\n","plt.tight_layout\n","plt.show()\n","\n","y_pred = Net.predict(X_val)\n","TP, TN, FP, FN = 0, 0, 0, 0\n","for i in range(len(y_val)):\n","    if y_pred[i] == 0 and  y_val[i] == 0:\n","        TN += 1\n","    elif y_pred[i] == 1 and  y_val[i] == 0:\n","        FP += 1\n","    elif y_pred[i] == 0 and  y_val[i] == 1:\n","        FN += 1\n","    elif y_pred[i] == 1 and  y_val[i] == 1:\n","        TP += 1\n","\n","print(TP, FP)\n","print(FN, TN)\n","\n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity,\\\n","     \"specificity = \", specificity)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Validation fold :  1\n","iteration 0 / 5000: loss 0.711279\n","iteration 100 / 5000: loss 0.124619\n","iteration 200 / 5000: loss 0.034071\n","iteration 300 / 5000: loss 0.025180\n","iteration 400 / 5000: loss 0.013143\n","iteration 500 / 5000: loss 0.004656\n","iteration 600 / 5000: loss 0.002899\n","iteration 700 / 5000: loss 0.002190\n","iteration 800 / 5000: loss 0.001793\n","iteration 900 / 5000: loss 0.001315\n","iteration 1000 / 5000: loss 0.001102\n","iteration 1100 / 5000: loss 0.000993\n","iteration 1200 / 5000: loss 0.000747\n","iteration 1300 / 5000: loss 0.000686\n","iteration 1400 / 5000: loss 0.000646\n","iteration 1500 / 5000: loss 0.000486\n","iteration 1600 / 5000: loss 0.000489\n","iteration 1700 / 5000: loss 0.000419\n","iteration 1800 / 5000: loss 0.000367\n","iteration 1900 / 5000: loss 0.000399\n","iteration 2000 / 5000: loss 0.000357\n","iteration 2100 / 5000: loss 0.000359\n","iteration 2200 / 5000: loss 0.000308\n","iteration 2300 / 5000: loss 0.000374\n","iteration 2400 / 5000: loss 0.000266\n","iteration 2500 / 5000: loss 0.000250\n","iteration 2600 / 5000: loss 0.000241\n","iteration 2700 / 5000: loss 0.000248\n","iteration 2800 / 5000: loss 0.000246\n","iteration 2900 / 5000: loss 0.000242\n","iteration 3000 / 5000: loss 0.000192\n","iteration 3100 / 5000: loss 0.000201\n","iteration 3200 / 5000: loss 0.000189\n","iteration 3300 / 5000: loss 0.000168\n","iteration 3400 / 5000: loss 0.000191\n","iteration 3500 / 5000: loss 0.000175\n","iteration 3600 / 5000: loss 0.000188\n","iteration 3700 / 5000: loss 0.000147\n","iteration 3800 / 5000: loss 0.000147\n","iteration 3900 / 5000: loss 0.000149\n","iteration 4000 / 5000: loss 0.000137\n","iteration 4100 / 5000: loss 0.000146\n","iteration 4200 / 5000: loss 0.000149\n","iteration 4300 / 5000: loss 0.000171\n","iteration 4400 / 5000: loss 0.000119\n","iteration 4500 / 5000: loss 0.000136\n","iteration 4600 / 5000: loss 0.000091\n","iteration 4700 / 5000: loss 0.000118\n","iteration 4800 / 5000: loss 0.000105\n","iteration 4900 / 5000: loss 0.000138\n","Validation fold :  2\n","iteration 0 / 5000: loss 0.722766\n","iteration 100 / 5000: loss 0.089657\n","iteration 200 / 5000: loss 0.029760\n","iteration 300 / 5000: loss 0.011774\n","iteration 400 / 5000: loss 0.005505\n","iteration 500 / 5000: loss 0.002659\n","iteration 600 / 5000: loss 0.002181\n","iteration 700 / 5000: loss 0.001470\n","iteration 800 / 5000: loss 0.001153\n","iteration 900 / 5000: loss 0.001054\n","iteration 1000 / 5000: loss 0.000727\n","iteration 1100 / 5000: loss 0.000834\n","iteration 1200 / 5000: loss 0.000679\n","iteration 1300 / 5000: loss 0.000530\n","iteration 1400 / 5000: loss 0.000526\n","iteration 1500 / 5000: loss 0.000467\n","iteration 1600 / 5000: loss 0.000430\n","iteration 1700 / 5000: loss 0.000517\n","iteration 1800 / 5000: loss 0.000410\n","iteration 1900 / 5000: loss 0.000331\n","iteration 2000 / 5000: loss 0.000281\n","iteration 2100 / 5000: loss 0.000280\n","iteration 2200 / 5000: loss 0.000265\n","iteration 2300 / 5000: loss 0.000259\n","iteration 2400 / 5000: loss 0.000191\n","iteration 2500 / 5000: loss 0.000220\n","iteration 2600 / 5000: loss 0.000221\n","iteration 2700 / 5000: loss 0.000205\n","iteration 2800 / 5000: loss 0.000187\n","iteration 2900 / 5000: loss 0.000194\n","iteration 3000 / 5000: loss 0.000161\n","iteration 3100 / 5000: loss 0.000179\n","iteration 3200 / 5000: loss 0.000176\n","iteration 3300 / 5000: loss 0.000167\n","iteration 3400 / 5000: loss 0.000173\n","iteration 3500 / 5000: loss 0.000150\n","iteration 3600 / 5000: loss 0.000150\n","iteration 3700 / 5000: loss 0.000145\n","iteration 3800 / 5000: loss 0.000130\n","iteration 3900 / 5000: loss 0.000149\n","iteration 4000 / 5000: loss 0.000118\n","iteration 4100 / 5000: loss 0.000143\n","iteration 4200 / 5000: loss 0.000130\n","iteration 4300 / 5000: loss 0.000102\n","iteration 4400 / 5000: loss 0.000129\n","iteration 4500 / 5000: loss 0.000114\n","iteration 4600 / 5000: loss 0.000103\n","iteration 4700 / 5000: loss 0.000099\n","iteration 4800 / 5000: loss 0.000091\n","iteration 4900 / 5000: loss 0.000100\n","Validation fold :  3\n","iteration 0 / 5000: loss 0.703951\n","iteration 100 / 5000: loss 0.097906\n","iteration 200 / 5000: loss 0.035252\n","iteration 300 / 5000: loss 0.012653\n","iteration 400 / 5000: loss 0.006943\n","iteration 500 / 5000: loss 0.004345\n","iteration 600 / 5000: loss 0.002230\n","iteration 700 / 5000: loss 0.002033\n","iteration 800 / 5000: loss 0.001682\n","iteration 900 / 5000: loss 0.001152\n","iteration 1000 / 5000: loss 0.001042\n","iteration 1100 / 5000: loss 0.000661\n","iteration 1200 / 5000: loss 0.000712\n","iteration 1300 / 5000: loss 0.000591\n","iteration 1400 / 5000: loss 0.000567\n","iteration 1500 / 5000: loss 0.000585\n","iteration 1600 / 5000: loss 0.000509\n","iteration 1700 / 5000: loss 0.000409\n","iteration 1800 / 5000: loss 0.000403\n","iteration 1900 / 5000: loss 0.000356\n","iteration 2000 / 5000: loss 0.000350\n","iteration 2100 / 5000: loss 0.000344\n","iteration 2200 / 5000: loss 0.000340\n","iteration 2300 / 5000: loss 0.000294\n","iteration 2400 / 5000: loss 0.000283\n","iteration 2500 / 5000: loss 0.000198\n","iteration 2600 / 5000: loss 0.000191\n","iteration 2700 / 5000: loss 0.000208\n","iteration 2800 / 5000: loss 0.000216\n","iteration 2900 / 5000: loss 0.000185\n","iteration 3000 / 5000: loss 0.000185\n","iteration 3100 / 5000: loss 0.000200\n","iteration 3200 / 5000: loss 0.000191\n","iteration 3300 / 5000: loss 0.000188\n","iteration 3400 / 5000: loss 0.000176\n","iteration 3500 / 5000: loss 0.000201\n","iteration 3600 / 5000: loss 0.000158\n","iteration 3700 / 5000: loss 0.000153\n","iteration 3800 / 5000: loss 0.000118\n","iteration 3900 / 5000: loss 0.000131\n","iteration 4000 / 5000: loss 0.000158\n","iteration 4100 / 5000: loss 0.000149\n","iteration 4200 / 5000: loss 0.000125\n","iteration 4300 / 5000: loss 0.000116\n","iteration 4400 / 5000: loss 0.000156\n","iteration 4500 / 5000: loss 0.000112\n","iteration 4600 / 5000: loss 0.000103\n","iteration 4700 / 5000: loss 0.000098\n","iteration 4800 / 5000: loss 0.000112\n","iteration 4900 / 5000: loss 0.000115\n","Validation fold :  4\n","iteration 0 / 5000: loss 0.693041\n","iteration 100 / 5000: loss 0.118872\n","iteration 200 / 5000: loss 0.030127\n","iteration 300 / 5000: loss 0.012436\n","iteration 400 / 5000: loss 0.006131\n","iteration 500 / 5000: loss 0.003281\n","iteration 600 / 5000: loss 0.002176\n","iteration 700 / 5000: loss 0.001559\n","iteration 800 / 5000: loss 0.001339\n","iteration 900 / 5000: loss 0.001102\n","iteration 1000 / 5000: loss 0.000860\n","iteration 1100 / 5000: loss 0.000861\n","iteration 1200 / 5000: loss 0.000819\n","iteration 1300 / 5000: loss 0.000666\n","iteration 1400 / 5000: loss 0.000766\n","iteration 1500 / 5000: loss 0.000441\n","iteration 1600 / 5000: loss 0.000454\n","iteration 1700 / 5000: loss 0.000420\n","iteration 1800 / 5000: loss 0.000326\n","iteration 1900 / 5000: loss 0.000379\n","iteration 2000 / 5000: loss 0.000342\n","iteration 2100 / 5000: loss 0.000271\n","iteration 2200 / 5000: loss 0.000238\n","iteration 2300 / 5000: loss 0.000247\n","iteration 2400 / 5000: loss 0.000241\n","iteration 2500 / 5000: loss 0.000269\n","iteration 2600 / 5000: loss 0.000258\n","iteration 2700 / 5000: loss 0.000233\n","iteration 2800 / 5000: loss 0.000196\n","iteration 2900 / 5000: loss 0.000194\n","iteration 3000 / 5000: loss 0.000208\n","iteration 3100 / 5000: loss 0.000191\n","iteration 3200 / 5000: loss 0.000164\n","iteration 3300 / 5000: loss 0.000173\n","iteration 3400 / 5000: loss 0.000173\n","iteration 3500 / 5000: loss 0.000165\n","iteration 3600 / 5000: loss 0.000151\n","iteration 3700 / 5000: loss 0.000157\n","iteration 3800 / 5000: loss 0.000121\n","iteration 3900 / 5000: loss 0.000145\n","iteration 4000 / 5000: loss 0.000144\n","iteration 4100 / 5000: loss 0.000149\n","iteration 4200 / 5000: loss 0.000142\n","iteration 4300 / 5000: loss 0.000111\n","iteration 4400 / 5000: loss 0.000128\n","iteration 4500 / 5000: loss 0.000122\n","iteration 4600 / 5000: loss 0.000104\n","iteration 4700 / 5000: loss 0.000117\n","iteration 4800 / 5000: loss 0.000101\n","iteration 4900 / 5000: loss 0.000096\n","Validation fold :  5\n","iteration 0 / 5000: loss 0.691253\n","iteration 100 / 5000: loss 0.101914\n","iteration 200 / 5000: loss 0.028000\n","iteration 300 / 5000: loss 0.010044\n","iteration 400 / 5000: loss 0.004952\n","iteration 500 / 5000: loss 0.004265\n","iteration 600 / 5000: loss 0.002396\n","iteration 700 / 5000: loss 0.001821\n","iteration 800 / 5000: loss 0.001408\n","iteration 900 / 5000: loss 0.001017\n","iteration 1000 / 5000: loss 0.001007\n","iteration 1100 / 5000: loss 0.000821\n","iteration 1200 / 5000: loss 0.000559\n","iteration 1300 / 5000: loss 0.000531\n","iteration 1400 / 5000: loss 0.000516\n","iteration 1500 / 5000: loss 0.000509\n","iteration 1600 / 5000: loss 0.000517\n","iteration 1700 / 5000: loss 0.000422\n","iteration 1800 / 5000: loss 0.000341\n","iteration 1900 / 5000: loss 0.000391\n","iteration 2000 / 5000: loss 0.000290\n","iteration 2100 / 5000: loss 0.000306\n","iteration 2200 / 5000: loss 0.000230\n","iteration 2300 / 5000: loss 0.000240\n","iteration 2400 / 5000: loss 0.000244\n","iteration 2500 / 5000: loss 0.000165\n","iteration 2600 / 5000: loss 0.000237\n","iteration 2700 / 5000: loss 0.000216\n","iteration 2800 / 5000: loss 0.000205\n","iteration 2900 / 5000: loss 0.000172\n","iteration 3000 / 5000: loss 0.000181\n","iteration 3100 / 5000: loss 0.000165\n","iteration 3200 / 5000: loss 0.000194\n","iteration 3300 / 5000: loss 0.000177\n","iteration 3400 / 5000: loss 0.000203\n","iteration 3500 / 5000: loss 0.000165\n","iteration 3600 / 5000: loss 0.000130\n","iteration 3700 / 5000: loss 0.000158\n","iteration 3800 / 5000: loss 0.000116\n","iteration 3900 / 5000: loss 0.000127\n","iteration 4000 / 5000: loss 0.000140\n","iteration 4100 / 5000: loss 0.000138\n","iteration 4200 / 5000: loss 0.000103\n","iteration 4300 / 5000: loss 0.000118\n","iteration 4400 / 5000: loss 0.000112\n","iteration 4500 / 5000: loss 0.000093\n","iteration 4600 / 5000: loss 0.000108\n","iteration 4700 / 5000: loss 0.000115\n","iteration 4800 / 5000: loss 0.000111\n","iteration 4900 / 5000: loss 0.000105\n","1.0 0.9608938547486033\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["196 0\n","0 162\n","accuracy =  1.0 sensitivity =  1.0 specificity =  1.0\n"],"name":"stdout"}]}]}