{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q4,Q5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NyBWyhGIacuxrFKTTpfrgSkONuatOjjS","authorship_tag":"ABX9TyMs6yTOnemqt1Bquf1xwcGo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VSwGTg93I9EX"},"source":["# Q4 - RBFNN"]},{"cell_type":"code","metadata":{"id":"9Z1qWt4-I7Gd","executionInfo":{"status":"ok","timestamp":1606844335977,"user_tz":-330,"elapsed":1231,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["import numpy as np\n","from scipy.io import loadmat\n","from sklearn.cluster import KMeans\n","\n","mat_contents = loadmat('/content/drive/MyDrive/2018AAPS1242H_NNFL (Assignment 2)/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"LN8KMpJXJIit","executionInfo":{"status":"ok","timestamp":1606844338567,"user_tz":-330,"elapsed":1141,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["def init_data():\n","    X = np.array(data[:2148, :-1], dtype = float)\n","    y = np.array(data[:2148, -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","def gaussian(x,center,sigma,beta):\n","    return np.exp(-beta * (np.linalg.norm(x - center)) ** 2)\n","\n","def multi_quadric(x, center, sigma, beta):\n","    return ((np.linalg.norm(x - center)) ** 2 + sigma ** 2) ** 0.5\n","\n","def linear(x, center, sigma, beta):\n","    return np.linalg.norm(x - center)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"xb615gLrJMTV","executionInfo":{"status":"ok","timestamp":1606844341536,"user_tz":-330,"elapsed":1175,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["X_tot, y_tot = init_data()\n","\n","train_X = X_tot[ : 1600]\n","train_y = y_tot[ : 1600]\n","test_X = X_tot[1600 : 2148]\n","test_y = y_tot[1600 : 2148]\n","\n","def fit_rbf(train_X, train_y, test_X, test_y):\n","    km = KMeans(n_clusters=550)\n","\n","    y_km = km.fit_predict(train_X)\n","    centers = km.cluster_centers_\n","    labels = km.predict(train_X)\n","\n","    sigma = np.zeros((len(centers), 1))\n","    beta = np.zeros((len(centers), 1))\n","    cluster_size = np.zeros((len(centers), 1))\n","\n","    for i in range(len(train_X)):\n","        sigma[labels[i]] += np.linalg.norm(train_X[i] - centers[labels[i]])\n","        cluster_size[labels[i]] += 1\n","\n","    sigma /= cluster_size\n","    beta = 1 / 2 * (sigma * sigma + 1e-6)\n","\n","    H = np.zeros((len(train_X), len(centers)))\n","\n","    for i in range(len(train_X)):\n","        for j in range(len(centers)):\n","            H[i, j] = linear(train_X[i], centers[j], sigma[j], beta[j])\n","\n","    W = np.dot(np.linalg.pinv(H), train_y)\n","\n","    #Test run\n","    H_test = np.zeros([len(test_X), len(centers)])\n","    for i in range(len(test_X)):\n","        for j in range(len(centers)):\n","            H_test[i, j] = linear(test_X[i], centers[j], sigma[j], beta[j])\n","\n","    y_pred = np.dot(H_test, W)\n","    for i in range(len(y_pred)):\n","        y_pred[i] = 1 if y_pred[i]>=0.5 else 0\n","        \n","    accuracy = 0    \n","    for i in range(len(y_pred)):\n","        if y_pred[i] == test_y[i]:\n","            accuracy +=1\n","    accuracy /= len(y_pred)\n","    print(accuracy)\n","    return y_pred, accuracy"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLc_k_2eJT4u","executionInfo":{"status":"ok","timestamp":1606844361015,"user_tz":-330,"elapsed":17482,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"c5505ce8-5815-4a2a-ef3a-562b00fcbbec"},"source":["y_pred, _ = fit_rbf(train_X, train_y, test_X, test_y)\n","split = 358\n","for i in range(len(y_pred)):\n","    y_pred[i] = 1 if y_pred[i] > 0.5 else 0\n","\n","TP, TN, FP, FN = 0,0,0,0 \n","\n","for i in range(len(test_X)):\n","    \n","    if y_pred[i] == 1 and test_y[i] == 1:\n","        TP += 1\n","    \n","    elif y_pred[i] == 0 and test_y[i] == 0:\n","        TN += 1\n","        \n","    elif y_pred[i] == 1 and test_y[i] == 0:\n","        FP += 1\n","        \n","    elif y_pred[i] == 0 and test_y[i] == 1:\n","        FN += 1\n","        \n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)\n","print(TP, FP)\n","print(FN, TN)\n","avg_acc = 0"],"execution_count":19,"outputs":[{"output_type":"stream","text":["0.9507299270072993\n","accuracy =  0.9507299270072993 sensitivity =  0.9680851063829787 specificity =  0.9323308270676691\n","273 18\n","9 248\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0YEGXuPJiol","executionInfo":{"status":"ok","timestamp":1606844451286,"user_tz":-330,"elapsed":84424,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"d548839d-492b-42d3-b29e-6227e9487c47"},"source":["# K - fold cross validation\n","\n","for k in range(5):\n","    X = X_tot[0 : 1790]\n","    y = y_tot[0 : 1790]\n","    X_val = X_tot[1790 :]\n","    y_val = y_tot[1790 :]\n","    _, acc = fit_rbf(X, y, X_val, y_val)\n","    avg_acc += acc\n","    X_tot[0 : split] = X_val\n","    X_tot[split : ] = X\n","    y_tot[0 : split] = y_val\n","    y_tot[split : ] = y\n","\n","avg_acc /= 5\n","print(avg_acc)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["0.9608938547486033\n","0.9497206703910615\n","0.9608938547486033\n","0.9636871508379888\n","0.946927374301676\n","0.9564245810055866\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pVoQjvFILO5x"},"source":["# Q5 - Stacked Autoencoder"]},{"cell_type":"code","metadata":{"id":"4tfOopyNLVOP","executionInfo":{"status":"ok","timestamp":1606844457824,"user_tz":-330,"elapsed":1282,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["import numpy as np\n","from scipy.io import loadmat\n","from sklearn.preprocessing import normalize\n","\n","#Load data, shuffle and normalize\n","mat_contents = loadmat('/content/drive/MyDrive/2018AAPS1242H_NNFL (Assignment 2)/Data/data5.mat')\n","data = mat_contents['x']\n","np.random.shuffle(data)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"nIJG0qeaLZH1","executionInfo":{"status":"ok","timestamp":1606844460826,"user_tz":-330,"elapsed":1283,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["def init_data():\n","    X = np.array(data[ : , :-1], dtype = float)\n","    y = np.array(data[ : , -1], dtype = int)\n","    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n","    return X, y\n","\n","X, y = init_data()\n","\n","#Hold out method of model evaluation\n","X_train, y_train = X[ :int(0.7 * len(X))], y[ :int(0.7 * len(X))]\n","X_val, y_val = X[ int(0.7 * len(X)): ], y[ int(0.7 * len(X)): ]\n","\n","alpha = 0.5\n","\n","#Sigmoid activation function\n","def sigmoid(x, derivative=False):\n","        if (derivative == True):\n","            return x * (1 - x)\n","        return 1 / (1 + np.exp(-x))"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkaGYohLLe40","executionInfo":{"status":"ok","timestamp":1606844464166,"user_tz":-330,"elapsed":1146,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["class NeuralNetwork(object):\n","    def __init__(self, sizes):\n","        \n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.W = {}\n","        self.a = {}\n","        self.b = {}\n","        \n","        #Initialize Weights\n","        for i in range(1, self.num_layers):\n","            self.W[i] = np.random.randn(self.sizes[i-1], self.sizes[i])\n","            \n","        #Initialize biases\n","        for i in range(1, self.num_layers):\n","            self.b[i] = np.random.randn(self.sizes[i], 1)\n","        \n","        #Initialize activations\n","        for i in range(1, self.num_layers):\n","            self.a[i] = np.zeros([self.sizes[i], 1])\n","        \n","    #Forward pass to compute scores\n","    def forward_pass(self, X):\n","        \n","        self.a[0] = X\n","        \n","        for i in range(1, self.num_layers):\n","            self.a[i] = sigmoid(np.dot(self.W[i].T, self.a[i-1]) + self.b[i])\n","\n","        return self.a[self.num_layers-1] \n","    \n","    #Backward pass to update weights\n","    def backward_pass(self, X, Y, output):\n","        \n","        self.d = {}\n","        self.d_output = (Y - output) * sigmoid(output, derivative=True)\n","        self.d[self.num_layers-1] = self.d_output\n","        \n","        #Derivatives of the layers wrt loss\n","        for i in range(self.num_layers-1, 1, -1):\n","            self.d[i-1] = np.dot(self.W[i], self.d[i]) * sigmoid(self.a[i-1], derivative=True)\n","        \n","        #Updating weights\n","        for i in range(1, self.num_layers-1):\n","            self.W[i] += alpha * np.dot(self.a[i-1], self.d[i].T)\n","            \n","        #Updating biases\n","        for i in range(1, self.num_layers-1):\n","            self.b[i] += alpha * self.d[i]\n","\n","    #Training helper function   \n","    def train(self, X, Y):\n","        X = np.reshape(X, (len(X), 1))\n","        output = self.forward_pass(X)\n","        self.backward_pass(X, Y, output)\n","\n","    #Get weights    \n","    def get_W(self):\n","        return self.W\n","    \n","    #Load specified weights\n","    def load_W(self, W):\n","        self.W = W\n","\n","    #Scores computation for given input    \n","    def get_a(self, x):\n","        x = np.reshape(x, (len(x), 1))\n","        self.forward_pass(x)\n","        return self.a\n","    \n","    #Helper function for autoencoder chaining\n","    def load_a(self, a):\n","        self.a = a"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXRxM1lgLm13","executionInfo":{"status":"ok","timestamp":1606844468980,"user_tz":-330,"elapsed":1086,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}}},"source":["#Loss function\n","def calc_loss(NN,x ,y):\n","    \n","    loss = 0\n","    for i in range(len(x)):\n","        x_ = np.reshape(x[i], (len(x[i]), 1))\n","        loss += 0.5 / len(x) * np.sum((y[i] - NN.forward_pass(x_)) ** 2)\n","    \n","    return loss"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXFHy7SMLqOW","executionInfo":{"status":"ok","timestamp":1606845044869,"user_tz":-330,"elapsed":171807,"user":{"displayName":"ANIRUDH BHUPALARAO","photoUrl":"","userId":"03163692510675770346"}},"outputId":"0fdd1222-9f50-4dad-f6f7-9cac637daebb"},"source":["#Network initialization\n","autoencoder1 = NeuralNetwork([72, 60, 72])\n","autoencoder2 = NeuralNetwork([60,40,60])\n","autoencoder3 = NeuralNetwork([40, 30, 40])\n","NN = NeuralNetwork([72,60,40,30, 1])\n","\n","#Autoencoder 1 pretraining\n","for i in range(200):\n","    for j, row in enumerate(X_train):\n","        row = np.reshape(row, (72,1))\n","        autoencoder1.train(row, row)\n","        \n","    loss = calc_loss(autoencoder1, X_train, X_train)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","    \n","#Scores computation for autoencoder 1\n","autoencoder2_input = []\n","\n","for row in X_train:\n","    autoencoder2_input.append(autoencoder1.get_a(row)[1])\n","\n","autoencoder2_input = np.array(autoencoder2_input)\n","\n","\n","#Autoencoder 2 pretraining\n","for i in range(200):\n","    for j, row in enumerate(autoencoder2_input):\n","        row = np.reshape(row, (60,1))\n","        autoencoder2.train(row, row)\n","        \n","    loss = calc_loss(autoencoder2, autoencoder2_input, autoencoder2_input)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","\n","\n","#Scores computation for autoencoder 2\n","autoencoder3_input = []\n","\n","for row in autoencoder2_input:\n","    autoencoder3_input.append(autoencoder2.get_a(row)[1])\n","\n","autoencoder3_input = np.array(autoencoder3_input)\n","\n","#Autoencoder 3 pretraining\n","for i in range(200):\n","    for j, row in enumerate(autoencoder3_input):\n","        row = np.reshape(row, (40,1))\n","        autoencoder3.train(row, row)\n","        \n","    loss = calc_loss(autoencoder3, autoencoder3_input, autoencoder3_input)\n","    print(\"Epoch {}, Loss {}\".format(i, loss))\n","\n","#Final network weight initialization\n","W1 = autoencoder1.get_W()[1]\n","W2 = autoencoder2.get_W()[1]\n","W3 = autoencoder3.get_W()[1]\n","W_final = {}\n","W_final[1] = W1\n","W_final[2] = W2\n","W_final[3] = W3\n","W_final[4] = np.random.randn(30, 1)\n","NN.load_W(W_final)\n","\n","#Training loop\n","for i in range(500):\n","    print(\"Epoch: \", i)\n","\n","    for j in range(len(X_train)):\n","        NN.train(X_train[j], y_train[j])\n","\n","TP,TN,FP,FN = 0,0,0,0\n","\n","for i in range(len(X_val)):\n","\n","    x = np.reshape(X_val[i], (len(X_val[i]), 1))\n","    x = NN.forward_pass(x)\n","    p = 0 if x[0] < 0.5 else 1\n","\n","    if p == 1 and y_val[i] == 1:\n","        TP += 1\n","    elif p == 0 and y_val[i] == 0:\n","        TN += 1\n","    elif p == 1 and y_val[i] == 0:\n","        FP += 1\n","    elif p == 0 and y_val[i] == 1:\n","        FN += 1\n","\n","print(TP, FP)\n","print(FN, TN)\n","\n","accuracy = (TP + TN) / (TP + TN + FP + FN)\n","sensitivity = TP / (TP + FN)\n","specificity = TN / (TN + FP)\n","\n","print(\"accuracy = \", accuracy, \"sensitivity = \", sensitivity, \"specificity = \", specificity)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, Loss 3339.025915856628\n","Epoch 1, Loss 3326.2679325529452\n","Epoch 2, Loss 3340.876994889057\n","Epoch 3, Loss 3313.390342411993\n","Epoch 4, Loss 3321.9774578712795\n","Epoch 5, Loss 3314.5031602344484\n","Epoch 6, Loss 3307.7184207676532\n","Epoch 7, Loss 3313.340751933394\n","Epoch 8, Loss 3304.1540276625674\n","Epoch 9, Loss 3296.332250078851\n","Epoch 10, Loss 3288.0381705429336\n","Epoch 11, Loss 3289.686366845842\n","Epoch 12, Loss 3282.652062995858\n","Epoch 13, Loss 3281.1490200468843\n","Epoch 14, Loss 3279.657528856027\n","Epoch 15, Loss 3280.079488330787\n","Epoch 16, Loss 3276.5932409725665\n","Epoch 17, Loss 3276.460686144868\n","Epoch 18, Loss 3274.1843236011487\n","Epoch 19, Loss 3270.7694109579\n","Epoch 20, Loss 3269.93882863624\n","Epoch 21, Loss 3269.8871189672577\n","Epoch 22, Loss 3263.706167355111\n","Epoch 23, Loss 3264.102234059048\n","Epoch 24, Loss 3262.4364602774717\n","Epoch 25, Loss 3258.6445640004486\n","Epoch 26, Loss 3256.5808290467444\n","Epoch 27, Loss 3256.681072112148\n","Epoch 28, Loss 3256.2440183278873\n","Epoch 29, Loss 3253.66364828347\n","Epoch 30, Loss 3253.8788828413426\n","Epoch 31, Loss 3254.1556251793986\n","Epoch 32, Loss 3251.494854478723\n","Epoch 33, Loss 3255.1985700261494\n","Epoch 34, Loss 3252.1103319970744\n","Epoch 35, Loss 3251.456738162903\n","Epoch 36, Loss 3248.6662001274044\n","Epoch 37, Loss 3246.965510583714\n","Epoch 38, Loss 3246.253112379078\n","Epoch 39, Loss 3242.813151927922\n","Epoch 40, Loss 3241.7838755345774\n","Epoch 41, Loss 3245.134418288753\n","Epoch 42, Loss 3244.62569858072\n","Epoch 43, Loss 3243.663217071002\n","Epoch 44, Loss 3243.0787359638534\n","Epoch 45, Loss 3240.0023236806132\n","Epoch 46, Loss 3241.0084650906747\n","Epoch 47, Loss 3240.068586257878\n","Epoch 48, Loss 3241.46153384175\n","Epoch 49, Loss 3239.561083064852\n","Epoch 50, Loss 3242.092233596507\n","Epoch 51, Loss 3241.8977959118806\n","Epoch 52, Loss 3241.021673186058\n","Epoch 53, Loss 3236.983588746212\n","Epoch 54, Loss 3239.2588355515345\n","Epoch 55, Loss 3236.7208350424435\n","Epoch 56, Loss 3240.4337278459907\n","Epoch 57, Loss 3237.958817473268\n","Epoch 58, Loss 3237.709664637946\n","Epoch 59, Loss 3238.256032240917\n","Epoch 60, Loss 3235.262116685463\n","Epoch 61, Loss 3236.178821651741\n","Epoch 62, Loss 3236.5841644804614\n","Epoch 63, Loss 3236.5613081400525\n","Epoch 64, Loss 3237.254981079978\n","Epoch 65, Loss 3237.352900280977\n","Epoch 66, Loss 3235.042277946168\n","Epoch 67, Loss 3234.7702219140506\n","Epoch 68, Loss 3233.7200884884246\n","Epoch 69, Loss 3234.287255841579\n","Epoch 70, Loss 3230.3853100723636\n","Epoch 71, Loss 3232.8014894567928\n","Epoch 72, Loss 3232.3109072203274\n","Epoch 73, Loss 3233.2046534774413\n","Epoch 74, Loss 3229.584812211163\n","Epoch 75, Loss 3231.0021967996304\n","Epoch 76, Loss 3234.0940352163207\n","Epoch 77, Loss 3231.4196784183086\n","Epoch 78, Loss 3230.5599095508423\n","Epoch 79, Loss 3228.6661978161874\n","Epoch 80, Loss 3231.111273472094\n","Epoch 81, Loss 3231.878385974851\n","Epoch 82, Loss 3231.6430154344052\n","Epoch 83, Loss 3228.8729827094758\n","Epoch 84, Loss 3230.116627192238\n","Epoch 85, Loss 3229.9561809116867\n","Epoch 86, Loss 3230.5506279515475\n","Epoch 87, Loss 3231.311626640908\n","Epoch 88, Loss 3231.459265211492\n","Epoch 89, Loss 3230.63793447226\n","Epoch 90, Loss 3229.920736157711\n","Epoch 91, Loss 3228.679234249487\n","Epoch 92, Loss 3232.0867215653\n","Epoch 93, Loss 3232.15642041837\n","Epoch 94, Loss 3233.0285147169475\n","Epoch 95, Loss 3230.4719160265918\n","Epoch 96, Loss 3228.3405699488158\n","Epoch 97, Loss 3231.977874847254\n","Epoch 98, Loss 3231.5166112986503\n","Epoch 99, Loss 3229.4304395463328\n","Epoch 100, Loss 3229.7381044582153\n","Epoch 101, Loss 3229.397138776201\n","Epoch 102, Loss 3231.200511848949\n","Epoch 103, Loss 3229.567246199333\n","Epoch 104, Loss 3228.254556138651\n","Epoch 105, Loss 3227.6908496337082\n","Epoch 106, Loss 3228.3585163334114\n","Epoch 107, Loss 3226.6809055922686\n","Epoch 108, Loss 3229.8231199980114\n","Epoch 109, Loss 3229.964077396196\n","Epoch 110, Loss 3228.559596354612\n","Epoch 111, Loss 3228.169866746826\n","Epoch 112, Loss 3227.2090860127673\n","Epoch 113, Loss 3228.932610346051\n","Epoch 114, Loss 3230.356068847073\n","Epoch 115, Loss 3231.781721965624\n","Epoch 116, Loss 3228.8265038253503\n","Epoch 117, Loss 3227.3877921154904\n","Epoch 118, Loss 3228.7747594647717\n","Epoch 119, Loss 3227.352789516473\n","Epoch 120, Loss 3228.865208645255\n","Epoch 121, Loss 3226.630332032655\n","Epoch 122, Loss 3227.1775099513834\n","Epoch 123, Loss 3224.9746515482007\n","Epoch 124, Loss 3229.2041176008247\n","Epoch 125, Loss 3225.7959427070014\n","Epoch 126, Loss 3226.279149049252\n","Epoch 127, Loss 3226.035704911297\n","Epoch 128, Loss 3224.8706346867834\n","Epoch 129, Loss 3225.533164132156\n","Epoch 130, Loss 3222.940867676963\n","Epoch 131, Loss 3224.0439230975217\n","Epoch 132, Loss 3225.273493225579\n","Epoch 133, Loss 3226.3191377667035\n","Epoch 134, Loss 3227.1290346478677\n","Epoch 135, Loss 3225.533577863839\n","Epoch 136, Loss 3226.9122073948447\n","Epoch 137, Loss 3225.4738695951974\n","Epoch 138, Loss 3222.827884694562\n","Epoch 139, Loss 3223.3326963691557\n","Epoch 140, Loss 3223.3682533835104\n","Epoch 141, Loss 3225.538517421988\n","Epoch 142, Loss 3223.536644050733\n","Epoch 143, Loss 3223.2857620406335\n","Epoch 144, Loss 3226.397887713001\n","Epoch 145, Loss 3228.031328057155\n","Epoch 146, Loss 3227.5085236188465\n","Epoch 147, Loss 3225.9870120843166\n","Epoch 148, Loss 3228.104997170581\n","Epoch 149, Loss 3226.7128624627476\n","Epoch 150, Loss 3227.3894891802574\n","Epoch 151, Loss 3226.0176773003977\n","Epoch 152, Loss 3224.466502324665\n","Epoch 153, Loss 3225.7636232256627\n","Epoch 154, Loss 3224.4592577893864\n","Epoch 155, Loss 3224.9549626556845\n","Epoch 156, Loss 3223.205272459834\n","Epoch 157, Loss 3224.317360292669\n","Epoch 158, Loss 3223.456207970332\n","Epoch 159, Loss 3224.9296239662763\n","Epoch 160, Loss 3223.069101468428\n","Epoch 161, Loss 3222.6364034970843\n","Epoch 162, Loss 3222.676334358867\n","Epoch 163, Loss 3223.570960056759\n","Epoch 164, Loss 3224.182710855409\n","Epoch 165, Loss 3224.207132297103\n","Epoch 166, Loss 3224.5085657331633\n","Epoch 167, Loss 3224.3947305432775\n","Epoch 168, Loss 3224.836179149931\n","Epoch 169, Loss 3226.490477146494\n","Epoch 170, Loss 3223.834063045766\n","Epoch 171, Loss 3224.095250573209\n","Epoch 172, Loss 3223.036532362668\n","Epoch 173, Loss 3223.1306325877317\n","Epoch 174, Loss 3224.296662833734\n","Epoch 175, Loss 3226.264551117228\n","Epoch 176, Loss 3223.1132329000034\n","Epoch 177, Loss 3223.5171942431975\n","Epoch 178, Loss 3222.3524394653473\n","Epoch 179, Loss 3222.996975450574\n","Epoch 180, Loss 3222.3087441802677\n","Epoch 181, Loss 3223.9432723775008\n","Epoch 182, Loss 3224.5510568216127\n","Epoch 183, Loss 3224.963661884148\n","Epoch 184, Loss 3222.7483448440507\n","Epoch 185, Loss 3223.1137033013374\n","Epoch 186, Loss 3222.666217306728\n","Epoch 187, Loss 3222.809821164879\n","Epoch 188, Loss 3223.060996668692\n","Epoch 189, Loss 3223.910644058819\n","Epoch 190, Loss 3224.240401531352\n","Epoch 191, Loss 3224.1797525454967\n","Epoch 192, Loss 3222.207795036183\n","Epoch 193, Loss 3224.3371856552144\n","Epoch 194, Loss 3224.6816306538135\n","Epoch 195, Loss 3224.9439031956085\n","Epoch 196, Loss 3223.1316486998953\n","Epoch 197, Loss 3224.5072067283886\n","Epoch 198, Loss 3222.5073003615958\n","Epoch 199, Loss 3222.68890411408\n","Epoch 0, Loss 7.0206500246610215\n","Epoch 1, Loss 6.354048156858523\n","Epoch 2, Loss 6.219617245466684\n","Epoch 3, Loss 6.157823957745626\n","Epoch 4, Loss 6.071969252790626\n","Epoch 5, Loss 6.039291819298589\n","Epoch 6, Loss 5.993546842190757\n","Epoch 7, Loss 5.977478502131331\n","Epoch 8, Loss 5.94450737206061\n","Epoch 9, Loss 5.913505249050234\n","Epoch 10, Loss 5.920077825891057\n","Epoch 11, Loss 5.905666585128421\n","Epoch 12, Loss 5.89298460252576\n","Epoch 13, Loss 5.887478667465862\n","Epoch 14, Loss 5.878231082564247\n","Epoch 15, Loss 5.864788731249174\n","Epoch 16, Loss 5.863706820397646\n","Epoch 17, Loss 5.854261753753231\n","Epoch 18, Loss 5.857676388714986\n","Epoch 19, Loss 5.856398079405754\n","Epoch 20, Loss 5.85591955800908\n","Epoch 21, Loss 5.856886993207476\n","Epoch 22, Loss 5.849149424834439\n","Epoch 23, Loss 5.848190680418985\n","Epoch 24, Loss 5.83331728399583\n","Epoch 25, Loss 5.8339283925040615\n","Epoch 26, Loss 5.832308905393722\n","Epoch 27, Loss 5.831497788310107\n","Epoch 28, Loss 5.8321251230743325\n","Epoch 29, Loss 5.833206302205753\n","Epoch 30, Loss 5.837489949383875\n","Epoch 31, Loss 5.839053600236945\n","Epoch 32, Loss 5.843198619386311\n","Epoch 33, Loss 5.8469813880210335\n","Epoch 34, Loss 5.849546532970884\n","Epoch 35, Loss 5.850567167131178\n","Epoch 36, Loss 5.850059531545349\n","Epoch 37, Loss 5.848561830090381\n","Epoch 38, Loss 5.8469224864048535\n","Epoch 39, Loss 5.845694279715488\n","Epoch 40, Loss 5.844371853679847\n","Epoch 41, Loss 5.842078284750606\n","Epoch 42, Loss 5.838995806981268\n","Epoch 43, Loss 5.83783370791529\n","Epoch 44, Loss 5.840669257859848\n","Epoch 45, Loss 5.840327384390315\n","Epoch 46, Loss 5.841108684155525\n","Epoch 47, Loss 5.8412457387350685\n","Epoch 48, Loss 5.841422669053861\n","Epoch 49, Loss 5.841202642104549\n","Epoch 50, Loss 5.840954459437164\n","Epoch 51, Loss 5.840879856080556\n","Epoch 52, Loss 5.8407551791750825\n","Epoch 53, Loss 5.84055277899074\n","Epoch 54, Loss 5.840241062466408\n","Epoch 55, Loss 5.839664387050851\n","Epoch 56, Loss 5.838633388263767\n","Epoch 57, Loss 5.836346980738065\n","Epoch 58, Loss 5.834244438532932\n","Epoch 59, Loss 5.8318633809396045\n","Epoch 60, Loss 5.830759249426174\n","Epoch 61, Loss 5.8307153954183315\n","Epoch 62, Loss 5.8316271070320225\n","Epoch 63, Loss 5.832462162851612\n","Epoch 64, Loss 5.833173505040471\n","Epoch 65, Loss 5.83360214428579\n","Epoch 66, Loss 5.833874692609717\n","Epoch 67, Loss 5.834044836535627\n","Epoch 68, Loss 5.834445790718942\n","Epoch 69, Loss 5.834555803946297\n","Epoch 70, Loss 5.835005265797808\n","Epoch 71, Loss 5.834212105404641\n","Epoch 72, Loss 5.8341273575432515\n","Epoch 73, Loss 5.834250452392987\n","Epoch 74, Loss 5.834060856600955\n","Epoch 75, Loss 5.83321274346284\n","Epoch 76, Loss 5.8331138393680835\n","Epoch 77, Loss 5.8322680380656235\n","Epoch 78, Loss 5.832224385869619\n","Epoch 79, Loss 5.83156822062564\n","Epoch 80, Loss 5.830730966090484\n","Epoch 81, Loss 5.829127115152348\n","Epoch 82, Loss 5.827638916294636\n","Epoch 83, Loss 5.827986537039615\n","Epoch 84, Loss 5.828131073673262\n","Epoch 85, Loss 5.828380331374276\n","Epoch 86, Loss 5.827158548539988\n","Epoch 87, Loss 5.829813198934603\n","Epoch 88, Loss 5.827071806699507\n","Epoch 89, Loss 5.831770369045322\n","Epoch 90, Loss 5.827594839738638\n","Epoch 91, Loss 5.832644371881174\n","Epoch 92, Loss 5.8336389144533705\n","Epoch 93, Loss 5.832265113307084\n","Epoch 94, Loss 5.835738322796208\n","Epoch 95, Loss 5.836177879349219\n","Epoch 96, Loss 5.83465673998697\n","Epoch 97, Loss 5.836672242476917\n","Epoch 98, Loss 5.836528883411765\n","Epoch 99, Loss 5.836679051960166\n","Epoch 100, Loss 5.8375694141945145\n","Epoch 101, Loss 5.837430394541322\n","Epoch 102, Loss 5.83882161676612\n","Epoch 103, Loss 5.837507803734684\n","Epoch 104, Loss 5.840863179202858\n","Epoch 105, Loss 5.836539146616737\n","Epoch 106, Loss 5.842447917936184\n","Epoch 107, Loss 5.837398836585588\n","Epoch 108, Loss 5.837852884422766\n","Epoch 109, Loss 5.842039958833032\n","Epoch 110, Loss 5.835226236812839\n","Epoch 111, Loss 5.840327911138517\n","Epoch 112, Loss 5.83410528748125\n","Epoch 113, Loss 5.835559233935445\n","Epoch 114, Loss 5.833329032801691\n","Epoch 115, Loss 5.832955938377348\n","Epoch 116, Loss 5.833155762447977\n","Epoch 117, Loss 5.834002690730856\n","Epoch 118, Loss 5.832037224399955\n","Epoch 119, Loss 5.830835081910391\n","Epoch 120, Loss 5.831853629583281\n","Epoch 121, Loss 5.830048064637835\n","Epoch 122, Loss 5.83112045199614\n","Epoch 123, Loss 5.831192501601571\n","Epoch 124, Loss 5.829955693636235\n","Epoch 125, Loss 5.830564032766072\n","Epoch 126, Loss 5.83126643047109\n","Epoch 127, Loss 5.8311166212572685\n","Epoch 128, Loss 5.8312823526564665\n","Epoch 129, Loss 5.831239182973179\n","Epoch 130, Loss 5.830665838126367\n","Epoch 131, Loss 5.83029653134\n","Epoch 132, Loss 5.82983126355134\n","Epoch 133, Loss 5.82932786125258\n","Epoch 134, Loss 5.828869065873304\n","Epoch 135, Loss 5.828489569120158\n","Epoch 136, Loss 5.828195226968685\n","Epoch 137, Loss 5.827946827449245\n","Epoch 138, Loss 5.827696468687512\n","Epoch 139, Loss 5.827425969933465\n","Epoch 140, Loss 5.827139010808223\n","Epoch 141, Loss 5.826829062544482\n","Epoch 142, Loss 5.826382185496091\n","Epoch 143, Loss 5.825934710419288\n","Epoch 144, Loss 5.826489130242761\n","Epoch 145, Loss 5.826939233220421\n","Epoch 146, Loss 5.827064111356468\n","Epoch 147, Loss 5.827176434972751\n","Epoch 148, Loss 5.827312275731561\n","Epoch 149, Loss 5.827488864297298\n","Epoch 150, Loss 5.827701942660713\n","Epoch 151, Loss 5.827899681944317\n","Epoch 152, Loss 5.827974802673391\n","Epoch 153, Loss 5.827906426478769\n","Epoch 154, Loss 5.827733714069348\n","Epoch 155, Loss 5.827447565058792\n","Epoch 156, Loss 5.827024431250119\n","Epoch 157, Loss 5.826508944347787\n","Epoch 158, Loss 5.826055772725715\n","Epoch 159, Loss 5.8258005979803436\n","Epoch 160, Loss 5.825671282476598\n","Epoch 161, Loss 5.825549727692573\n","Epoch 162, Loss 5.825422929504043\n","Epoch 163, Loss 5.825318150260138\n","Epoch 164, Loss 5.82523510555393\n","Epoch 165, Loss 5.825169092586885\n","Epoch 166, Loss 5.825123338191483\n","Epoch 167, Loss 5.825094460683028\n","Epoch 168, Loss 5.825077417985328\n","Epoch 169, Loss 5.825070397743936\n","Epoch 170, Loss 5.825070806451145\n","Epoch 171, Loss 5.8250766302626475\n","Epoch 172, Loss 5.825087719585004\n","Epoch 173, Loss 5.825104387039654\n","Epoch 174, Loss 5.825127767432696\n","Epoch 175, Loss 5.82515980143588\n","Epoch 176, Loss 5.8252023287888965\n","Epoch 177, Loss 5.825256311898616\n","Epoch 178, Loss 5.82532045640894\n","Epoch 179, Loss 5.825389988865796\n","Epoch 180, Loss 5.825456566588198\n","Epoch 181, Loss 5.8255089073176585\n","Epoch 182, Loss 5.8255322827345895\n","Epoch 183, Loss 5.825503509403568\n","Epoch 184, Loss 5.825378753451637\n","Epoch 185, Loss 5.825097239641193\n","Epoch 186, Loss 5.8247011807434435\n","Epoch 187, Loss 5.82413202116323\n","Epoch 188, Loss 5.823219024087034\n","Epoch 189, Loss 5.82253832915684\n","Epoch 190, Loss 5.822207553385667\n","Epoch 191, Loss 5.8219376959185425\n","Epoch 192, Loss 5.82174289173363\n","Epoch 193, Loss 5.821675951488529\n","Epoch 194, Loss 5.8216831146195975\n","Epoch 195, Loss 5.8217393191851325\n","Epoch 196, Loss 5.821825385804261\n","Epoch 197, Loss 5.821893998986366\n","Epoch 198, Loss 5.821998312770302\n","Epoch 199, Loss 5.822153215648772\n","Epoch 0, Loss 2.187134374323691\n","Epoch 1, Loss 2.07441250070207\n","Epoch 2, Loss 2.031348288242519\n","Epoch 3, Loss 2.0068000179854986\n","Epoch 4, Loss 1.986482900913477\n","Epoch 5, Loss 1.973876725608989\n","Epoch 6, Loss 1.9665938968368932\n","Epoch 7, Loss 1.9615003709677605\n","Epoch 8, Loss 1.958283794244383\n","Epoch 9, Loss 1.956108043794662\n","Epoch 10, Loss 1.9544997589451891\n","Epoch 11, Loss 1.9533469205258487\n","Epoch 12, Loss 1.9525205689834877\n","Epoch 13, Loss 1.951898587514199\n","Epoch 14, Loss 1.9514065466891963\n","Epoch 15, Loss 1.9510022723113698\n","Epoch 16, Loss 1.950661013981868\n","Epoch 17, Loss 1.9503674657129466\n","Epoch 18, Loss 1.9501115486953404\n","Epoch 19, Loss 1.9498862017373377\n","Epoch 20, Loss 1.9496862186003037\n","Epoch 21, Loss 1.9495076105192883\n","Epoch 22, Loss 1.9493472300165269\n","Epoch 23, Loss 1.949202529562085\n","Epoch 24, Loss 1.949071395621402\n","Epoch 25, Loss 1.9489520296476097\n","Epoch 26, Loss 1.9488428613286988\n","Epoch 27, Loss 1.9487424849144201\n","Epoch 28, Loss 1.9486496111410574\n","Epoch 29, Loss 1.9485630274316563\n","Epoch 30, Loss 1.9484815587609219\n","Epoch 31, Loss 1.9484040212401648\n","Epoch 32, Loss 1.9483291607925262\n","Epoch 33, Loss 1.9482555743635326\n","Epoch 34, Loss 1.9481816400868095\n","Epoch 35, Loss 1.9481056115787287\n","Epoch 36, Loss 1.9480264890520984\n","Epoch 37, Loss 1.9479471209546753\n","Epoch 38, Loss 1.947878233723583\n","Epoch 39, Loss 1.9478299484213741\n","Epoch 40, Loss 1.947796659990061\n","Epoch 41, Loss 1.9477679274989996\n","Epoch 42, Loss 1.9477394451457275\n","Epoch 43, Loss 1.9477106441542429\n","Epoch 44, Loss 1.9476818577850181\n","Epoch 45, Loss 1.9476534447114533\n","Epoch 46, Loss 1.9476256406048882\n","Epoch 47, Loss 1.9475985699248282\n","Epoch 48, Loss 1.9475722757718679\n","Epoch 49, Loss 1.9475467422008983\n","Epoch 50, Loss 1.9475219072216565\n","Epoch 51, Loss 1.9474976678160791\n","Epoch 52, Loss 1.9474738776848928\n","Epoch 53, Loss 1.9474503372900658\n","Epoch 54, Loss 1.9474267743454554\n","Epoch 55, Loss 1.9474028109494395\n","Epoch 56, Loss 1.9473779107657154\n","Epoch 57, Loss 1.9473512963439366\n","Epoch 58, Loss 1.9473218258646954\n","Epoch 59, Loss 1.947287832678934\n","Epoch 60, Loss 1.9472469913374926\n","Epoch 61, Loss 1.947196368290534\n","Epoch 62, Loss 1.9471323405505692\n","Epoch 63, Loss 1.9470466830484583\n","Epoch 64, Loss 1.9469058324271722\n","Epoch 65, Loss 1.9465566314761134\n","Epoch 66, Loss 1.9457248556514832\n","Epoch 67, Loss 1.9447753592027641\n","Epoch 68, Loss 1.9439464891504572\n","Epoch 69, Loss 1.9434683289736028\n","Epoch 70, Loss 1.9431058425749475\n","Epoch 71, Loss 1.9426677527245828\n","Epoch 72, Loss 1.9420874122794856\n","Epoch 73, Loss 1.9415903158454115\n","Epoch 74, Loss 1.9411125783843204\n","Epoch 75, Loss 1.9406310225337553\n","Epoch 76, Loss 1.9401408765203991\n","Epoch 77, Loss 1.9396462080635135\n","Epoch 78, Loss 1.9390953703926876\n","Epoch 79, Loss 1.93840693898169\n","Epoch 80, Loss 1.9376815439533603\n","Epoch 81, Loss 1.9370862697714866\n","Epoch 82, Loss 1.9366122946703497\n","Epoch 83, Loss 1.9362287697384772\n","Epoch 84, Loss 1.9359176048517652\n","Epoch 85, Loss 1.9356637836605701\n","Epoch 86, Loss 1.9354550670488357\n","Epoch 87, Loss 1.9352817704156269\n","Epoch 88, Loss 1.935136080133652\n","Epoch 89, Loss 1.9350117964705655\n","Epoch 90, Loss 1.9349041884885876\n","Epoch 91, Loss 1.9348097312699668\n","Epoch 92, Loss 1.9347258082847372\n","Epoch 93, Loss 1.9346504631026753\n","Epoch 94, Loss 1.9345822173238096\n","Epoch 95, Loss 1.9345199421900587\n","Epoch 96, Loss 1.934462767675654\n","Epoch 97, Loss 1.9344100167100862\n","Epoch 98, Loss 1.934361156462033\n","Epoch 99, Loss 1.9343157616795752\n","Epoch 100, Loss 1.9342734869706957\n","Epoch 101, Loss 1.9342340459728162\n","Epoch 102, Loss 1.9341971959276\n","Epoch 103, Loss 1.9341627264812884\n","Epoch 104, Loss 1.9341304517216587\n","Epoch 105, Loss 1.934100204619225\n","Epoch 106, Loss 1.9340718331949338\n","Epoch 107, Loss 1.934045197889373\n","Epoch 108, Loss 1.9340201697487398\n","Epoch 109, Loss 1.9339966291589454\n","Epoch 110, Loss 1.9339744649467685\n","Epoch 111, Loss 1.9339535737268816\n","Epoch 112, Loss 1.9339338594122182\n","Epoch 113, Loss 1.9339152328288332\n","Epoch 114, Loss 1.9338976113924473\n","Epoch 115, Loss 1.9338809188147505\n","Epoch 116, Loss 1.9338650848172128\n","Epoch 117, Loss 1.9338500448380398\n","Epoch 118, Loss 1.9338357397251513\n","Epoch 119, Loss 1.9338221154134245\n","Epoch 120, Loss 1.9338091225886378\n","Epoch 121, Loss 1.9337967163431153\n","Epoch 122, Loss 1.9337848558291406\n","Epoch 123, Loss 1.9337735039165491\n","Epoch 124, Loss 1.933762626860174\n","Epoch 125, Loss 1.9337521939819093\n","Epoch 126, Loss 1.9337421773710453\n","Epoch 127, Loss 1.93373255160516\n","Epoch 128, Loss 1.933723293493043\n","Epoch 129, Loss 1.9337143818400468\n","Epoch 130, Loss 1.9337057972355771\n","Epoch 131, Loss 1.9336975218619208\n","Epoch 132, Loss 1.9336895393233318\n","Epoch 133, Loss 1.9336818344938171\n","Epoch 134, Loss 1.9336743933822462\n","Epoch 135, Loss 1.9336672030130369\n","Epoch 136, Loss 1.9336602513210643\n","Epoch 137, Loss 1.933653527059107\n","Epoch 138, Loss 1.9336470197165312\n","Epoch 139, Loss 1.933640719447921\n","Epoch 140, Loss 1.9336346170103536\n","Epoch 141, Loss 1.9336287037083797\n","Epoch 142, Loss 1.933622971345675\n","Epoch 143, Loss 1.9336174121824574\n","Epoch 144, Loss 1.93361201889807\n","Epoch 145, Loss 1.9336067845578484\n","Epoch 146, Loss 1.9336017025838699\n","Epoch 147, Loss 1.9335967667289877\n","Epoch 148, Loss 1.9335919710536869\n","Epoch 149, Loss 1.933587309905451\n","Epoch 150, Loss 1.9335827779002703\n","Epoch 151, Loss 1.9335783699059468\n","Epoch 152, Loss 1.9335740810270712\n","Epoch 153, Loss 1.9335699065913337\n","Epoch 154, Loss 1.9335658421370938\n","Epoch 155, Loss 1.9335618834019541\n","Epoch 156, Loss 1.9335580263123102\n","Epoch 157, Loss 1.9335542669736412\n","Epoch 158, Loss 1.9335506016615764\n","Epoch 159, Loss 1.9335470268135333\n","Epoch 160, Loss 1.9335435390209474\n","Epoch 161, Loss 1.9335401350219465\n","Epoch 162, Loss 1.9335368116945633\n","Epoch 163, Loss 1.933533566050204\n","Epoch 164, Loss 1.9335303952276244\n","Epoch 165, Loss 1.9335272964871228\n","Epoch 166, Loss 1.9335242672050896\n","Epoch 167, Loss 1.9335213048688122\n","Epoch 168, Loss 1.9335184070715405\n","Epoch 169, Loss 1.9335155715077823\n","Epoch 170, Loss 1.9335127959688352\n","Epoch 171, Loss 1.9335100783384995\n","Epoch 172, Loss 1.9335074165890067\n","Epoch 173, Loss 1.9335048087771127\n","Epoch 174, Loss 1.9335022530403836\n","Epoch 175, Loss 1.9334997475936468\n","Epoch 176, Loss 1.9334972907255719\n","Epoch 177, Loss 1.9334948807954078\n","Epoch 178, Loss 1.933492516229916\n","Epoch 179, Loss 1.9334901955203476\n","Epoch 180, Loss 1.9334879172196022\n","Epoch 181, Loss 1.9334856799395608\n","Epoch 182, Loss 1.9334834823484148\n","Epoch 183, Loss 1.9334813231682202\n","Epoch 184, Loss 1.9334792011725057\n","Epoch 185, Loss 1.9334771151840078\n","Epoch 186, Loss 1.933475064072499\n","Epoch 187, Loss 1.9334730467527\n","Epoch 188, Loss 1.933471062182314\n","Epoch 189, Loss 1.9334691093601242\n","Epoch 190, Loss 1.933467187324196\n","Epoch 191, Loss 1.9334652951501299\n","Epoch 192, Loss 1.9334634319494337\n","Epoch 193, Loss 1.9334615968679367\n","Epoch 194, Loss 1.933459789084294\n","Epoch 195, Loss 1.933458007808554\n","Epoch 196, Loss 1.9334562522807919\n","Epoch 197, Loss 1.9334545217697856\n","Epoch 198, Loss 1.9334528155718071\n","Epoch 199, Loss 1.9334511330094102\n","Epoch:  0\n","Epoch:  1\n","Epoch:  2\n","Epoch:  3\n","Epoch:  4\n","Epoch:  5\n","Epoch:  6\n","Epoch:  7\n","Epoch:  8\n","Epoch:  9\n","Epoch:  10\n","Epoch:  11\n","Epoch:  12\n","Epoch:  13\n","Epoch:  14\n","Epoch:  15\n","Epoch:  16\n","Epoch:  17\n","Epoch:  18\n","Epoch:  19\n","Epoch:  20\n","Epoch:  21\n","Epoch:  22\n","Epoch:  23\n","Epoch:  24\n","Epoch:  25\n","Epoch:  26\n","Epoch:  27\n","Epoch:  28\n","Epoch:  29\n","Epoch:  30\n","Epoch:  31\n","Epoch:  32\n","Epoch:  33\n","Epoch:  34\n","Epoch:  35\n","Epoch:  36\n","Epoch:  37\n","Epoch:  38\n","Epoch:  39\n","Epoch:  40\n","Epoch:  41\n","Epoch:  42\n","Epoch:  43\n","Epoch:  44\n","Epoch:  45\n","Epoch:  46\n","Epoch:  47\n","Epoch:  48\n","Epoch:  49\n","Epoch:  50\n","Epoch:  51\n","Epoch:  52\n","Epoch:  53\n","Epoch:  54\n","Epoch:  55\n","Epoch:  56\n","Epoch:  57\n","Epoch:  58\n","Epoch:  59\n","Epoch:  60\n","Epoch:  61\n","Epoch:  62\n","Epoch:  63\n","Epoch:  64\n","Epoch:  65\n","Epoch:  66\n","Epoch:  67\n","Epoch:  68\n","Epoch:  69\n","Epoch:  70\n","Epoch:  71\n","Epoch:  72\n","Epoch:  73\n","Epoch:  74\n","Epoch:  75\n","Epoch:  76\n","Epoch:  77\n","Epoch:  78\n","Epoch:  79\n","Epoch:  80\n","Epoch:  81\n","Epoch:  82\n","Epoch:  83\n","Epoch:  84\n","Epoch:  85\n","Epoch:  86\n","Epoch:  87\n","Epoch:  88\n","Epoch:  89\n","Epoch:  90\n","Epoch:  91\n","Epoch:  92\n","Epoch:  93\n","Epoch:  94\n","Epoch:  95\n","Epoch:  96\n","Epoch:  97\n","Epoch:  98\n","Epoch:  99\n","Epoch:  100\n","Epoch:  101\n","Epoch:  102\n","Epoch:  103\n","Epoch:  104\n","Epoch:  105\n","Epoch:  106\n","Epoch:  107\n","Epoch:  108\n","Epoch:  109\n","Epoch:  110\n","Epoch:  111\n","Epoch:  112\n","Epoch:  113\n","Epoch:  114\n","Epoch:  115\n","Epoch:  116\n","Epoch:  117\n","Epoch:  118\n","Epoch:  119\n","Epoch:  120\n","Epoch:  121\n","Epoch:  122\n","Epoch:  123\n","Epoch:  124\n","Epoch:  125\n","Epoch:  126\n","Epoch:  127\n","Epoch:  128\n","Epoch:  129\n","Epoch:  130\n","Epoch:  131\n","Epoch:  132\n","Epoch:  133\n","Epoch:  134\n","Epoch:  135\n","Epoch:  136\n","Epoch:  137\n","Epoch:  138\n","Epoch:  139\n","Epoch:  140\n","Epoch:  141\n","Epoch:  142\n","Epoch:  143\n","Epoch:  144\n","Epoch:  145\n","Epoch:  146\n","Epoch:  147\n","Epoch:  148\n","Epoch:  149\n","Epoch:  150\n","Epoch:  151\n","Epoch:  152\n","Epoch:  153\n","Epoch:  154\n","Epoch:  155\n","Epoch:  156\n","Epoch:  157\n","Epoch:  158\n","Epoch:  159\n","Epoch:  160\n","Epoch:  161\n","Epoch:  162\n","Epoch:  163\n","Epoch:  164\n","Epoch:  165\n","Epoch:  166\n","Epoch:  167\n","Epoch:  168\n","Epoch:  169\n","Epoch:  170\n","Epoch:  171\n","Epoch:  172\n","Epoch:  173\n","Epoch:  174\n","Epoch:  175\n","Epoch:  176\n","Epoch:  177\n","Epoch:  178\n","Epoch:  179\n","Epoch:  180\n","Epoch:  181\n","Epoch:  182\n","Epoch:  183\n","Epoch:  184\n","Epoch:  185\n","Epoch:  186\n","Epoch:  187\n","Epoch:  188\n","Epoch:  189\n","Epoch:  190\n","Epoch:  191\n","Epoch:  192\n","Epoch:  193\n","Epoch:  194\n","Epoch:  195\n","Epoch:  196\n","Epoch:  197\n","Epoch:  198\n","Epoch:  199\n","Epoch:  200\n","Epoch:  201\n","Epoch:  202\n","Epoch:  203\n","Epoch:  204\n","Epoch:  205\n","Epoch:  206\n","Epoch:  207\n","Epoch:  208\n","Epoch:  209\n","Epoch:  210\n","Epoch:  211\n","Epoch:  212\n","Epoch:  213\n","Epoch:  214\n","Epoch:  215\n","Epoch:  216\n","Epoch:  217\n","Epoch:  218\n","Epoch:  219\n","Epoch:  220\n","Epoch:  221\n","Epoch:  222\n","Epoch:  223\n","Epoch:  224\n","Epoch:  225\n","Epoch:  226\n","Epoch:  227\n","Epoch:  228\n","Epoch:  229\n","Epoch:  230\n","Epoch:  231\n","Epoch:  232\n","Epoch:  233\n","Epoch:  234\n","Epoch:  235\n","Epoch:  236\n","Epoch:  237\n","Epoch:  238\n","Epoch:  239\n","Epoch:  240\n","Epoch:  241\n","Epoch:  242\n","Epoch:  243\n","Epoch:  244\n","Epoch:  245\n","Epoch:  246\n","Epoch:  247\n","Epoch:  248\n","Epoch:  249\n","Epoch:  250\n","Epoch:  251\n","Epoch:  252\n","Epoch:  253\n","Epoch:  254\n","Epoch:  255\n","Epoch:  256\n","Epoch:  257\n","Epoch:  258\n","Epoch:  259\n","Epoch:  260\n","Epoch:  261\n","Epoch:  262\n","Epoch:  263\n","Epoch:  264\n","Epoch:  265\n","Epoch:  266\n","Epoch:  267\n","Epoch:  268\n","Epoch:  269\n","Epoch:  270\n","Epoch:  271\n","Epoch:  272\n","Epoch:  273\n","Epoch:  274\n","Epoch:  275\n","Epoch:  276\n","Epoch:  277\n","Epoch:  278\n","Epoch:  279\n","Epoch:  280\n","Epoch:  281\n","Epoch:  282\n","Epoch:  283\n","Epoch:  284\n","Epoch:  285\n","Epoch:  286\n","Epoch:  287\n","Epoch:  288\n","Epoch:  289\n","Epoch:  290\n","Epoch:  291\n","Epoch:  292\n","Epoch:  293\n","Epoch:  294\n","Epoch:  295\n","Epoch:  296\n","Epoch:  297\n","Epoch:  298\n","Epoch:  299\n","Epoch:  300\n","Epoch:  301\n","Epoch:  302\n","Epoch:  303\n","Epoch:  304\n","Epoch:  305\n","Epoch:  306\n","Epoch:  307\n","Epoch:  308\n","Epoch:  309\n","Epoch:  310\n","Epoch:  311\n","Epoch:  312\n","Epoch:  313\n","Epoch:  314\n","Epoch:  315\n","Epoch:  316\n","Epoch:  317\n","Epoch:  318\n","Epoch:  319\n","Epoch:  320\n","Epoch:  321\n","Epoch:  322\n","Epoch:  323\n","Epoch:  324\n","Epoch:  325\n","Epoch:  326\n","Epoch:  327\n","Epoch:  328\n","Epoch:  329\n","Epoch:  330\n","Epoch:  331\n","Epoch:  332\n","Epoch:  333\n","Epoch:  334\n","Epoch:  335\n","Epoch:  336\n","Epoch:  337\n","Epoch:  338\n","Epoch:  339\n","Epoch:  340\n","Epoch:  341\n","Epoch:  342\n","Epoch:  343\n","Epoch:  344\n","Epoch:  345\n","Epoch:  346\n","Epoch:  347\n","Epoch:  348\n","Epoch:  349\n","Epoch:  350\n","Epoch:  351\n","Epoch:  352\n","Epoch:  353\n","Epoch:  354\n","Epoch:  355\n","Epoch:  356\n","Epoch:  357\n","Epoch:  358\n","Epoch:  359\n","Epoch:  360\n","Epoch:  361\n","Epoch:  362\n","Epoch:  363\n","Epoch:  364\n","Epoch:  365\n","Epoch:  366\n","Epoch:  367\n","Epoch:  368\n","Epoch:  369\n","Epoch:  370\n","Epoch:  371\n","Epoch:  372\n","Epoch:  373\n","Epoch:  374\n","Epoch:  375\n","Epoch:  376\n","Epoch:  377\n","Epoch:  378\n","Epoch:  379\n","Epoch:  380\n","Epoch:  381\n","Epoch:  382\n","Epoch:  383\n","Epoch:  384\n","Epoch:  385\n","Epoch:  386\n","Epoch:  387\n","Epoch:  388\n","Epoch:  389\n","Epoch:  390\n","Epoch:  391\n","Epoch:  392\n","Epoch:  393\n","Epoch:  394\n","Epoch:  395\n","Epoch:  396\n","Epoch:  397\n","Epoch:  398\n","Epoch:  399\n","Epoch:  400\n","Epoch:  401\n","Epoch:  402\n","Epoch:  403\n","Epoch:  404\n","Epoch:  405\n","Epoch:  406\n","Epoch:  407\n","Epoch:  408\n","Epoch:  409\n","Epoch:  410\n","Epoch:  411\n","Epoch:  412\n","Epoch:  413\n","Epoch:  414\n","Epoch:  415\n","Epoch:  416\n","Epoch:  417\n","Epoch:  418\n","Epoch:  419\n","Epoch:  420\n","Epoch:  421\n","Epoch:  422\n","Epoch:  423\n","Epoch:  424\n","Epoch:  425\n","Epoch:  426\n","Epoch:  427\n","Epoch:  428\n","Epoch:  429\n","Epoch:  430\n","Epoch:  431\n","Epoch:  432\n","Epoch:  433\n","Epoch:  434\n","Epoch:  435\n","Epoch:  436\n","Epoch:  437\n","Epoch:  438\n","Epoch:  439\n","Epoch:  440\n","Epoch:  441\n","Epoch:  442\n","Epoch:  443\n","Epoch:  444\n","Epoch:  445\n","Epoch:  446\n","Epoch:  447\n","Epoch:  448\n","Epoch:  449\n","Epoch:  450\n","Epoch:  451\n","Epoch:  452\n","Epoch:  453\n","Epoch:  454\n","Epoch:  455\n","Epoch:  456\n","Epoch:  457\n","Epoch:  458\n","Epoch:  459\n","Epoch:  460\n","Epoch:  461\n","Epoch:  462\n","Epoch:  463\n","Epoch:  464\n","Epoch:  465\n","Epoch:  466\n","Epoch:  467\n","Epoch:  468\n","Epoch:  469\n","Epoch:  470\n","Epoch:  471\n","Epoch:  472\n","Epoch:  473\n","Epoch:  474\n","Epoch:  475\n","Epoch:  476\n","Epoch:  477\n","Epoch:  478\n","Epoch:  479\n","Epoch:  480\n","Epoch:  481\n","Epoch:  482\n","Epoch:  483\n","Epoch:  484\n","Epoch:  485\n","Epoch:  486\n","Epoch:  487\n","Epoch:  488\n","Epoch:  489\n","Epoch:  490\n","Epoch:  491\n","Epoch:  492\n","Epoch:  493\n","Epoch:  494\n","Epoch:  495\n","Epoch:  496\n","Epoch:  497\n","Epoch:  498\n","Epoch:  499\n","278 24\n","40 303\n","accuracy =  0.9007751937984496 sensitivity =  0.8742138364779874 specificity =  0.926605504587156\n"],"name":"stdout"}]}]}